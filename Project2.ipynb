{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nltk\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.tokenize import sent_tokenize\n",
    "from wordcloud import WordCloud\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "import re\n",
    "from sklearn.metrics import pairwise_distances\n",
    "from sacremoses import MosesDetokenizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import string\n",
    "from autocorrect import Speller\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.util import ngrams\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Season</th>\n",
       "      <th>Episode</th>\n",
       "      <th>Character</th>\n",
       "      <th>Line</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>Boys</td>\n",
       "      <td>School day, school day, teacher's golden ru...\\n</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>Kyle</td>\n",
       "      <td>Ah, damn it! My little brother's trying to fol...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>Ike</td>\n",
       "      <td>Zeeponanner.\\n</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>Kyle</td>\n",
       "      <td>Ike, you can't come to school with me. \\n</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>Cartman</td>\n",
       "      <td>Yeah, go home you little dildo.\\n</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Season Episode Character                                               Line\n",
       "0      1       1      Boys   School day, school day, teacher's golden ru...\\n\n",
       "1      1       1      Kyle  Ah, damn it! My little brother's trying to fol...\n",
       "2      1       1       Ike                                     Zeeponanner.\\n\n",
       "3      1       1      Kyle          Ike, you can't come to school with me. \\n\n",
       "4      1       1   Cartman                  Yeah, go home you little dildo.\\n"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Import South Park Script\n",
    "southpark = pd.read_csv(\"All-seasons.csv\")\n",
    "southpark.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cartman"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Season</th>\n",
       "      <th>Episode</th>\n",
       "      <th>Character</th>\n",
       "      <th>Line</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>Cartman</td>\n",
       "      <td>Yeah, go home you little dildo.\\n</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>Cartman</td>\n",
       "      <td>I know what it means!\\n</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>Cartman</td>\n",
       "      <td>I'm not telling you.\\n</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>Cartman</td>\n",
       "      <td>He-yeah, that's what Kyle's little brother is ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>Cartman</td>\n",
       "      <td>That's 'cause I was having these... bogus nigh...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Season Episode Character                                               Line\n",
       "4       1       1   Cartman                  Yeah, go home you little dildo.\\n\n",
       "8       1       1   Cartman                            I know what it means!\\n\n",
       "10      1       1   Cartman                             I'm not telling you.\\n\n",
       "13      1       1   Cartman  He-yeah, that's what Kyle's little brother is ...\n",
       "19      1       1   Cartman  That's 'cause I was having these... bogus nigh..."
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Dialogue spoken by Cartman\n",
    "cart = southpark['Character'] == 'Cartman'\n",
    "cartman = southpark[cart]\n",
    "cartman.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Season</th>\n",
       "      <th>Episode</th>\n",
       "      <th>Character</th>\n",
       "      <th>Line</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>Cartman</td>\n",
       "      <td>Yeah, go home you little dildo.\\n</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>Cartman</td>\n",
       "      <td>I know what it means!\\n</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>Cartman</td>\n",
       "      <td>I'm not telling you.\\n</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>Cartman</td>\n",
       "      <td>He-yeah, that's what Kyle's little brother is ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>Cartman</td>\n",
       "      <td>That's 'cause I was having these... bogus nigh...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Season Episode Character                                               Line\n",
       "0      1       1   Cartman                  Yeah, go home you little dildo.\\n\n",
       "1      1       1   Cartman                            I know what it means!\\n\n",
       "2      1       1   Cartman                             I'm not telling you.\\n\n",
       "3      1       1   Cartman  He-yeah, that's what Kyle's little brother is ...\n",
       "4      1       1   Cartman  That's 'cause I was having these... bogus nigh..."
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Reset index\n",
    "cartman = cartman.reset_index(drop=True)\n",
    "cartman.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0                     Yeah, go home you little dildo.\\n\n",
       "1                               I know what it means!\\n\n",
       "2                               I am not telling you.\\n\n",
       "3     He-yeah, that is what Kyle is little brother i...\n",
       "4     That is 'cause I was having these... bogus nig...\n",
       "5     Well, I dreamt that I was lying in my bed...  ...\n",
       "6                                               What?\\n\n",
       "7            No, it was just a dream, my mom said so.\\n\n",
       "8     Oh, shut up guys! You are just trying to make ...\n",
       "9                                           Kick ass.\\n\n",
       "10                                               Huh?\\n\n",
       "11    Eh, no, that, that was just a dream. And I am ...\n",
       "12                                                Oh!\\n\n",
       "13                                                Oh!\\n\n",
       "14         No! Uh-I mean, eh, why would they do that?\\n\n",
       "15                                                No!\\n\n",
       "16                                    Shut up, dildo!\\n\n",
       "17                                                Oh!\\n\n",
       "18    God damn it, they did not do anything to my as...\n",
       "19                                           Shut up!\\n\n",
       "20               Shut up you guys, it is not working.\\n\n",
       "21                      Somebody is baking brownies. \\n\n",
       "22    Heh, heh, that is a, that is, that is a little...\n",
       "23     Hah, hah. Mr. Hat yelled at you.  Ow! My ass! \\n\n",
       "24                                  Uh... Ow! My ass!\\n\n",
       "Name: Line_process, dtype: object"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Expand Contractions\n",
    "def decontracted(phrase):\n",
    "    # specific\n",
    "    phrase = re.sub(r\"won\\'t\", \"will not\", phrase)\n",
    "    phrase = re.sub(r\"can\\'t\", \"can not\", phrase)\n",
    "\n",
    "    # general\n",
    "    phrase = re.sub(r\"n\\'t\", \" not\", phrase)\n",
    "    phrase = re.sub(r\"\\'re\", \" are\", phrase)\n",
    "    phrase = re.sub(r\"\\'s\", \" is\", phrase)\n",
    "    phrase = re.sub(r\"\\'d\", \" would\", phrase)\n",
    "    phrase = re.sub(r\"\\'ll\", \" will\", phrase)\n",
    "    phrase = re.sub(r\"\\'t\", \" not\", phrase)\n",
    "    phrase = re.sub(r\"\\'ve\", \" have\", phrase)\n",
    "    phrase = re.sub(r\"\\'m\", \" am\", phrase)\n",
    "    return phrase\n",
    "cartman['Line_process'] = cartman['Line'].apply(decontracted)\n",
    "cartman['Line_process'].head(25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0                       yeah, go home you little dildo.\n",
       "1                                 i know what it means!\n",
       "2                                 i am not telling you.\n",
       "3     he-yeah, that is what kyle is little brother i...\n",
       "4     that is 'cause i was having these... bogus nig...\n",
       "5     well, i dreamt that i was lying in my bed... i...\n",
       "6                                                 what?\n",
       "7              no, it was just a dream, my mom said so.\n",
       "8     oh, shut up guys! you are just trying to make ...\n",
       "9                                             kick ass.\n",
       "10                                                 huh?\n",
       "11    eh, no, that, that was just a dream. and i am ...\n",
       "12                                                  oh!\n",
       "13                                                  oh!\n",
       "14           no! uh-i mean, eh, why would they do that?\n",
       "15                                                  no!\n",
       "16                                      shut up, dildo!\n",
       "17                                                  oh!\n",
       "18    god damn it, they did not do anything to my as...\n",
       "19                                             shut up!\n",
       "20                 shut up you guys, it is not working.\n",
       "21                         somebody is baking brownies.\n",
       "22    heh, heh, that is a, that is, that is a little...\n",
       "23         hah, hah. mr. hat yelled at you. ow! my ass!\n",
       "24                                    uh... ow! my ass!\n",
       "Name: Line_process, dtype: object"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Lowercase\n",
    "cartman['Line_process'] = cartman['Line_process'].apply(lambda x: \" \".join(x.lower() for x in x.split()))\n",
    "cartman['Line_process'].head(25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0                       yeah, go home you little dildo.\n",
       "1                                 i know what it means!\n",
       "2                                 i am not telling you.\n",
       "3     he-yeah, that is what kyle is little brother i...\n",
       "4     that is 'cause i was having these... bogus nig...\n",
       "5     well, i dreamt that i was lying in my bed... i...\n",
       "6                                                 what?\n",
       "7              no, it was just a dream, my mom said so.\n",
       "8     oh, shut up guys! you are just trying to make ...\n",
       "9                                             kick ass.\n",
       "10                                                 huh?\n",
       "11    eh, no, that, that was just a dream. and i am ...\n",
       "12                                                  oh!\n",
       "13                                                  oh!\n",
       "14           no! uh-i mean, eh, why would they do that?\n",
       "15                                                  no!\n",
       "16                                      shut up, dildo!\n",
       "17                                                  oh!\n",
       "18    god damn it, they did not do anything to my as...\n",
       "19                                             shut up!\n",
       "20                 shut up you guys, it is not working.\n",
       "21                         somebody is baking brownies.\n",
       "22    heh, heh, that is a, that is, that is a little...\n",
       "23         hah, hah. mr. hat yelled at you. ow! my ass!\n",
       "24                                    uh... ow! my ass!\n",
       "Name: Line_process, dtype: object"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Remove numbers\n",
    "cartman['Line_process'] = cartman['Line_process'].str.replace('\\d+', '')\n",
    "cartman['Line_process'].head(25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Spellchecker\n",
    "spell = Speller(lang='en')\n",
    "cartman[\"Line_process\"] = [' '.join([spell(i) for i in x.split()]) for x in cartman['Line_process']]\n",
    "cartman['Line_process'].head(25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Remove Stop Words\n",
    "from nltk.corpus import stopwords\n",
    "stop = stopwords.words('english')\n",
    "cartman['Line_process'] = cartman['Line_process'].apply(lambda x: \" \".join(x for x in x.split() if x not in stop))\n",
    "cartman['Line_process'].head(25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Remove Punctuation\n",
    "cartman['Line_process'] = cartman['Line_process'].str.replace('[^\\w\\s]','')\n",
    "cartman['Line_process'].head(25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Tokenize\n",
    "cartman['word_tokens'] = cartman['Line_process'].apply(word_tokenize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cartman['word_tokens'].head(25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Lemmatize\n",
    "def lemmatize_text(text):\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    return [lemmatizer.lemmatize(w) for w in text]\n",
    "cartman['word_tokens'].apply(lemmatize_text).head(25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Convert Word Tokens to String\n",
    "md = MosesDetokenizer(lang = 'en')\n",
    "cartman['token_string'] = cartman['word_tokens'].apply(lambda x: md.detokenize(x, return_str=True))\n",
    "cartman['token_string'].head(25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Wordcloud\n",
    "wordcloud_cartman = WordCloud().generate(cartman['word_tokens'].to_string())\n",
    "plt.figure(figsize=(16, 10))\n",
    "plt.imshow(wordcloud_cartman)\n",
    "plt.axis(\"off\")\n",
    "plt.savefig('Cartman_Wordcloud.png')\n",
    "plt.title('Cartman Wordcloud')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cartman_lists =  cartman['word_tokens']\n",
    "cartman_words = []\n",
    "for cartman_wordList in cartman_lists:\n",
    "    cartman_words += cartman_wordList\n",
    "cartman_bigram = ngrams(cartman_words,2)\n",
    "cartman_bigram_top = Counter(cartman_bigram).most_common(10)\n",
    "for word, count in cartman_bigram_top:\n",
    "    print(word, \":\", count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cartman_bi = pd.DataFrame(cartman_bigram_top, columns = ['word', 'frequency'])\n",
    "cartman_bi.plot(kind='bar', x='word', figsize=(16,10), title='Cartman Bigram Top Words')\n",
    "plt.savefig('Cartman_Bigram.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cartman_trigram = ngrams(cartman_words,3)\n",
    "cartman_trigram_top = Counter(cartman_trigram).most_common(10)\n",
    "for word, count in cartman_bigram_top:\n",
    "    print(word, \":\", count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cartman_tri = pd.DataFrame(cartman_trigram_top, columns = ['word', 'frequency'])\n",
    "cartman_tri.plot(kind='bar', x='word', figsize=(16,10), title='Cartman Trigram Top Words')\n",
    "plt.savefig('Cartman_Trigram.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Bag of Word Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cartman_vec = CountVectorizer()\n",
    "cartman_bag_of_words = cartman_vec.fit_transform(cartman['token_string'])\n",
    "cartman_sum_words = cartman_bag_of_words.sum(axis=0)\n",
    "cartman_words_freq = [(word, cartman_sum_words[0, idx]) for word, idx in cartman_vec.vocabulary_.items()]\n",
    "cartman_words_freq = np.array(sorted(cartman_words_freq, key = lambda x: x[1], reverse=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = 50\n",
    "plt.figure(figsize=(16, 10))\n",
    "plt.barh(-np.arange(n), cartman_words_freq[:n, 1].astype(float), height=.8)\n",
    "plt.yticks(ticks=-np.arange(n), labels=cartman_words_freq[:n, 0])\n",
    "plt.savefig('Cartman_BOW.png')\n",
    "plt.title('Cartman BoW Top Words')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create New Dataframe of Bag of Words Array\n",
    "cartman_bag = cartman_vec.fit_transform(cartman['token_string']).toarray()\n",
    "cartman_bag_features = cartman_vec.get_feature_names()\n",
    "cartman_bow = pd.DataFrame(cartman_bag, columns = cartman_bag_features)\n",
    "cartman_bow.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TF-IDF\n",
    "cartman_vect = TfidfVectorizer()\n",
    "cartman_tf = cartman_vect.fit_transform(cartman['token_string'])\n",
    "cartman_sum_tfidf = cartman_tf.sum(axis=0)\n",
    "cartman_tfidf_freq = [(word, cartman_sum_tfidf[0, idx]) for word, idx in cartman_vect.vocabulary_.items()]\n",
    "cartman_tfidf_freq = np.array(sorted(cartman_tfidf_freq, key = lambda x: x[1], reverse=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = 50\n",
    "plt.figure(figsize=(16, 10))\n",
    "plt.barh(-np.arange(n), cartman_tfidf_freq[:n, 1].astype(float), height=.8)\n",
    "plt.yticks(ticks=-np.arange(n), labels=cartman_tfidf_freq[:n, 0])\n",
    "plt.savefig('Cartman_TFIDF.png')\n",
    "plt.title('Cartman TFIDF Top Words')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create New Dataframe of TFIDF Array\n",
    "cartman_vectorize = cartman_vect.fit_transform(cartman['token_string']).toarray()\n",
    "cartman_tfidf_features = cartman_vect.get_feature_names()\n",
    "cartman_tfidf = pd.DataFrame(cartman_vectorize, columns = cartman_tfidf_features)\n",
    "cartman_tfidf.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Preprocessing Function\n",
    "def text_normalization(question):\n",
    "    #Preprocessing\n",
    "    #Expand Contractions\n",
    "    question = decontracted(question)\n",
    "    #Remove Punctuation\n",
    "    question = re.sub(r'[^\\w\\s]','',question)\n",
    "    #Lowercase\n",
    "    question = str(question).lower()\n",
    "    #Remove Numbers\n",
    "    question = ''.join(i for i in question if not i.isdigit())\n",
    "    #Spellchecker\n",
    "    question = spell(question)\n",
    "    #Tokenize\n",
    "    question = word_tokenize(question)\n",
    "    #Remove Stop Words\n",
    "    question = [i for i in question if i not in stop]\n",
    "    #Lemmatize\n",
    "    question = lemmatize_text(question)\n",
    "    #Detokenize\n",
    "    question = md.detokenize(question)\n",
    "    return question"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Kyle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Dialogue spoken by Kyle\n",
    "ky = southpark['Character'] == 'Kyle'\n",
    "kyle = southpark[ky]\n",
    "kyle.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Reset index\n",
    "kyle = kyle.reset_index(drop=True)\n",
    "kyle.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Expand Contractions\n",
    "kyle['Line_process'] = kyle['Line'].apply(decontracted)\n",
    "kyle['Line_process'].head(25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Lowercase\n",
    "kyle['Line_process'] = kyle['Line_process'].apply(lambda x: \" \".join(x.lower() for x in x.split()))\n",
    "kyle['Line_process'].head(25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Remove numbers\n",
    "kyle['Line_process'] = kyle['Line_process'].str.replace('\\d+', '')\n",
    "kyle['Line_process'].head(25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Spellchecker\n",
    "kyle[\"Line_process\"] = [' '.join([spell(i) for i in x.split()]) for x in kyle['Line_process']]\n",
    "kyle['Line_process'].head(25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Remove Stop Words\n",
    "kyle['Line_process'] = kyle['Line_process'].apply(lambda x: \" \".join(x for x in x.split() if x not in stop))\n",
    "kyle['Line_process'].head(25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Remove Punctuation\n",
    "kyle['Line_process'] = kyle['Line_process'].str.replace('[^\\w\\s]','')\n",
    "kyle['Line_process'].head(25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Tokenize\n",
    "kyle['word_tokens'] = kyle['Line_process'].apply(word_tokenize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kyle['word_tokens'].head(25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Lemmatize\n",
    "kyle['word_tokens'].apply(lemmatize_text).head(25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Convert Word Tokens to String\n",
    "kyle['token_string'] = kyle['word_tokens'].apply(lambda x: md.detokenize(x, return_str=True))\n",
    "kyle['token_string'].head(25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Wordcloud\n",
    "wordcloud_kyle = WordCloud().generate(kyle['word_tokens'].to_string())\n",
    "plt.figure(figsize=(16, 10))\n",
    "plt.imshow(wordcloud_kyle)\n",
    "plt.axis(\"off\")\n",
    "plt.savefig('Kyle_Wordcloud.png')\n",
    "plt.title('Kyle Wordcloud')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kyle_lists =  kyle['word_tokens']\n",
    "kyle_words = []\n",
    "for kyle_wordList in kyle_lists:\n",
    "    kyle_words += kyle_wordList\n",
    "kyle_bigram = ngrams(kyle_words,2)\n",
    "kyle_bigram_top = Counter(kyle_bigram).most_common(10)\n",
    "for word, count in kyle_bigram_top:\n",
    "    print(word, \":\", count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kyle_bi = pd.DataFrame(kyle_bigram_top, columns = ['word', 'frequency'])\n",
    "kyle_bi.plot(kind='bar', x='word', figsize=(16,10), title='Kyle Bigram Top Words')\n",
    "plt.savefig('Kyle_Bigram.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kyle_trigram = ngrams(kyle_words,3)\n",
    "kyle_trigram_top = Counter(kyle_trigram).most_common(10)\n",
    "for word, count in stan_trigram_top:\n",
    "    print(word, \":\", count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kyle_tri = pd.DataFrame(kyle_trigram_top, columns = ['word', 'frequency'])\n",
    "kyle_tri.plot(kind='bar', x='word', figsize=(16,10), title='Kyle Trigram Top Words')\n",
    "plt.savefig('Kyle_Trigram.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Bag of Word Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kyle_vec = CountVectorizer()\n",
    "kyle_bag = kyle_vec.fit_transform(kyle['token_string'])\n",
    "kyle_sum_words = kyle_bag.sum(axis=0)\n",
    "kyle_words_freq = [(word, kyle_sum_words[0, idx]) for word, idx in kyle_vec.vocabulary_.items()]\n",
    "kyle_words_freq = np.array(sorted(kyle_words_freq, key = lambda x: x[1], reverse=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = 50\n",
    "plt.figure(figsize=(16, 10))\n",
    "plt.barh(-np.arange(n), kyle_words_freq[:n, 1].astype(float), height=.8)\n",
    "plt.yticks(ticks=-np.arange(n), labels=kyle_words_freq[:n, 0])\n",
    "plt.savefig('Kyle_BOW.png')\n",
    "plt.title('Kyle BoW Top Words')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create New Dataframe of Bag of Words Array\n",
    "kyle_bag_of_words = kyle_vec.fit_transform(kyle['token_string']).toarray()\n",
    "kyle_features_bag = kyle_vec.get_feature_names()\n",
    "kyle_bow = pd.DataFrame(kyle_bag_of_words, columns = kyle_features_bag)\n",
    "kyle_bow.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TF-IDF\n",
    "kyle_vect = TfidfVectorizer()\n",
    "kyle_tfidf = kyle_vect.fit_transform(kyle['token_string'])\n",
    "kyle_sum_tfidf = kyle_tfidf.sum(axis=0)\n",
    "kyle_tfidf_freq = [(word, kyle_sum_tfidf[0, idx]) for word, idx in kyle_vect.vocabulary_.items()]\n",
    "kyle_tfidf_freq = np.array(sorted(kyle_tfidf_freq, key = lambda x: x[1], reverse=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = 50\n",
    "plt.figure(figsize=(16, 10))\n",
    "plt.barh(-np.arange(n), kyle_tfidf_freq[:n, 1].astype(float), height=.8)\n",
    "plt.yticks(ticks=-np.arange(n), labels=kyle_tfidf_freq[:n, 0])\n",
    "plt.savefig('Kyle_TFIDF.png')\n",
    "plt.title('Kyle TFIDF Top Words')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create New Dataframe of TFIDF Array\n",
    "kyle_vectorize = kyle_vect.fit_transform(kyle['token_string']).toarray()\n",
    "kyle_features_tfidf = kyle_vect.get_feature_names()\n",
    "kyle_tfidf = pd.DataFrame(kyle_vectorize, columns = kyle_features_tfidf)\n",
    "kyle_tfidf.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Dialogue spoken by Stan\n",
    "sta = southpark['Character'] == 'Stan'\n",
    "stan = southpark[sta]\n",
    "stan.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Reset index\n",
    "stan = stan.reset_index(drop=True)\n",
    "stan.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Expand Contractions\n",
    "stan['Line_process'] = stan['Line'].apply(decontracted)\n",
    "stan['Line_process'].head(25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Lowercase\n",
    "stan['Line_process'] = stan['Line_process'].apply(lambda x: \" \".join(x.lower() for x in x.split()))\n",
    "stan['Line_process'].head(25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Remove numbers\n",
    "stan['Line_process'] = stan['Line_process'].str.replace('\\d+', '')\n",
    "stan['Line_process'].head(25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Spellchecker\n",
    "stan[\"Line_process\"] = [' '.join([spell(i) for i in x.split()]) for x in stan['Line_process']]\n",
    "stan['Line_process'].head(25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Remove Stop Words\n",
    "stan['Line_process'] = stan['Line_process'].apply(lambda x: \" \".join(x for x in x.split() if x not in stop))\n",
    "stan['Line_process'].head(25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Remove Punctuation\n",
    "stan['Line_process'] = stan['Line_process'].str.replace('[^\\w\\s]','')\n",
    "stan['Line_process'].head(25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Tokenize\n",
    "stan['word_tokens'] = stan['Line_process'].apply(word_tokenize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stan['word_tokens'].head(25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Lemmatize\n",
    "stan['word_tokens'].apply(lemmatize_text).head(25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Convert Word Tokens to String\n",
    "stan['token_string'] = stan['word_tokens'].apply(lambda x: md.detokenize(x, return_str=True))\n",
    "stan['token_string'].head(25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Wordcloud\n",
    "wordcloud_stan = WordCloud().generate(stan['word_tokens'].to_string())\n",
    "plt.figure(figsize=(16, 10))\n",
    "plt.imshow(wordcloud_stan)\n",
    "plt.axis(\"off\")\n",
    "plt.savefig('Stan_Wordcloud.png')\n",
    "plt.title('Stan Wordcloud')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stan_lists =  stan['word_tokens']\n",
    "stan_words = []\n",
    "for stan_wordList in stan_lists:\n",
    "    stan_words += stan_wordList\n",
    "stan_bigram = ngrams(stan_words,2)\n",
    "stan_bigram_top = Counter(stan_bigram).most_common(10)\n",
    "for word, count in stan_bigram_top:\n",
    "    print(word, \":\", count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stan_bi = pd.DataFrame(stan_bigram_top, columns = ['word', 'frequency'])\n",
    "stan_bi.plot(kind='bar', x='word', figsize=(16,10), title='Stan Bigram Top Words')\n",
    "plt.savefig('Stan_Bigram.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stan_trigram = ngrams(stan_words,3)\n",
    "stan_trigram_top = Counter(stan_trigram).most_common(10)\n",
    "for word, count in stan_trigram_top:\n",
    "    print(word, \":\", count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stan_tri = pd.DataFrame(stan_trigram_top, columns = ['word', 'frequency'])\n",
    "stan_tri.plot(kind='bar', x='word', figsize=(16,10), title='Stan Trigram Top Words')\n",
    "plt.savefig('Stan_Trigram.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Bag of Word Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stan_vec = CountVectorizer()\n",
    "stan_bag_of_words = stan_vec.fit_transform(stan['token_string'])\n",
    "stan_sum_words = stan_bag_of_words.sum(axis=0)\n",
    "stan_words_freq = [(word, stan_sum_words[0, idx]) for word, idx in stan_vec.vocabulary_.items()]\n",
    "stan_words_freq = np.array(sorted(stan_words_freq, key = lambda x: x[1], reverse=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = 50\n",
    "plt.figure(figsize=(16, 10))\n",
    "plt.barh(-np.arange(n), stan_words_freq[:n, 1].astype(float), height=.8)\n",
    "plt.yticks(ticks=-np.arange(n), labels=stan_words_freq[:n, 0])\n",
    "plt.savefig('Stan_BOW.png')\n",
    "plt.title('Stan BoW Top Words')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create New Dataframe of Bag of Words Array\n",
    "stan_bag_of_words = stan_vec.fit_transform(stan['token_string']).toarray()\n",
    "stan_bow_features = stan_vec.get_feature_names()\n",
    "stan_bow = pd.DataFrame(stan_bag_of_words, columns = stan_bow_features)\n",
    "stan_bow.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TF-IDF\n",
    "stan_vect = TfidfVectorizer()\n",
    "stan_tfidf = stan_vect.fit_transform(stan['token_string'])\n",
    "stan_sum_tfidf = stan_tfidf.sum(axis=0)\n",
    "stan_tfidf_freq = [(word, stan_sum_tfidf[0, idx]) for word, idx in stan_vect.vocabulary_.items()]\n",
    "stan_tfidf_freq = np.array(sorted(stan_tfidf_freq, key = lambda x: x[1], reverse=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = 50\n",
    "plt.figure(figsize=(16, 10))\n",
    "plt.barh(-np.arange(n), stan_tfidf_freq[:n, 1].astype(float), height=.8)\n",
    "plt.yticks(ticks=-np.arange(n), labels=stan_tfidf_freq[:n, 0])\n",
    "plt.savefig('Stan_TFIDF.png')\n",
    "plt.title('Stan TFIDF Top Words')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create New Dataframe of TFIDF Array\n",
    "stan_vectorize = stan_vect.fit_transform(stan['token_string']).toarray()\n",
    "stan_tfidf_features = stan_vect.get_feature_names()\n",
    "stan_tfidf = pd.DataFrame(stan_vectorize, columns = stan_tfidf_features)\n",
    "stan_tfidf.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chatbots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Chat Function Using Bag of Words Model\n",
    "def cartman_chat_bow(question):\n",
    "    cartman_lemma = text_normalization(question)\n",
    "    cartman_bagg = cartman_vec.transform([cartman_lemma]).toarray()\n",
    "    cartman_cos = 1 - pairwise_distances(cartman_bow, cartman_bagg, metric = 'cosine')\n",
    "    cartman_index_val = cartman_cos.argmax()\n",
    "    return cartman['Line'].loc[cartman_index_val]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Chat Function Using TFIDF Model\n",
    "def cartman_chat_tfidf(question):\n",
    "    cartman_lem = text_normalization(question)\n",
    "    cartman_tf = cartman_vect.transform([cartman_lem]).toarray()\n",
    "    cartman_cosine = 1 - pairwise_distances(cartman_bow, cartman_tf, metric = 'cosine')\n",
    "    cartman_index_value = cartman_cosine.argmax()\n",
    "    return cartman['Line'].loc[cartman_index_value]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Chat Function Using Bag of Words Model\n",
    "def kyle_chat_bow(question):\n",
    "    kyle_lemma = text_normalization(question)\n",
    "    kyle_bagg = kyle_vec.transform([kyle_lemma]).toarray()\n",
    "    kyle_cos = 1 - pairwise_distances(kyle_bow, kyle_bagg, metric = 'cosine')\n",
    "    kyle_index_val = kyle_cos.argmax()\n",
    "    return kyle['Line'].loc[kyle_index_val]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Chat Function Using TFIDF Model\n",
    "def kyle_chat_tfidf(question):\n",
    "    kyle_lem = text_normalization(question)\n",
    "    kyle_tf = kyle_vect.transform([kyle_lem]).toarray()\n",
    "    kyle_cosine = 1 - pairwise_distances(kyle_bow, kyle_tf, metric = 'cosine')\n",
    "    kyle_index_value = kyle_cosine.argmax()\n",
    "    return kyle['Line'].loc[kyle_index_value]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Chat Function Using Bag of Words Model\n",
    "def stan_chat_bow(question):\n",
    "    stan_lemma = text_normalization(question)\n",
    "    stan_bagg = vec.transform([lemma]).toarray()\n",
    "    stan_cos = 1 - pairwise_distances(stan_bow, stan_bagg, metric = 'cosine')\n",
    "    stan_index_val = stan_cos.argmax()\n",
    "    return stan['Line'].loc[stan_index_val]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Chat Function Using TFIDF Model\n",
    "def stan_chat_tfidf(question):\n",
    "    stan_lem = text_normalization(question)\n",
    "    stan_tf = stan_vect.transform([stan_lem]).toarray()\n",
    "    stan_cosine = 1 - pairwise_distances(stan_bow, stan_tf, metric = 'cosine')\n",
    "    stan_index_value = stan_cosine.argmax()\n",
    "    return stan['Line'].loc[stan_index_value]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Chatbots Talking to Each Other"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "question = input('What is your question?')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
